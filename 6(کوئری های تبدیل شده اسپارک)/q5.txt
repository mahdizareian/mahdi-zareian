from decimal import *
from pyspark.sql.types import *
from pyspark.sql import functions as F
from pyspark.sql import *

lineitem = sqlContext.read.parquet("hdfs://namenode:8020/mahdi-parquet-data/lineitem.parquet")
orders = sqlContext.read.parquet("hdfs://namenode:8020/mahdi-parquet-data/orders.parquet")
supplier = sqlContext.read.parquet("hdfs://namenode:8020/mahdi-parquet-data/supplier.parquet")
nation = sqlContext.read.parquet("hdfs://namenode:8020/mahdi-parquet-data/nation.parquet")
region = sqlContext.read.parquet("hdfs://namenode:8020/mahdi-parquet-data/region.parquet")
customer = sqlContext.read.parquet("hdfs://namenode:8020/mahdi-parquet-data/customer.parquet")

func1=lambda x, y : x * (1-y)
orders_filter = orders.filter((orders.O_ORDERDATE >= "1997-06-09") & (orders.O_ORDERDATE < "1998-06-09"))

query5=region.filter(region.R_NAME == "ASIA")\
        .join(nation, region.R_REGIONKEY == nation.N_REGIONKEY)\
        .join(supplier, nation.N_NATIONKEY == supplier.S_NATIONKEY)\
        .join(lineitem, supplier.S_SUPPKEY == lineitem.L_SUPPKEY)\
        .join(orders_filter, lineitem.L_ORDERKEY == orders_filter.O_ORDERKEY)\
        .join(customer, orders_filter.O_CUSTKEY == customer.C_CUSTKEY)\
        .select(nation.N_NAME, func1(lineitem.L_EXTENDEDPRICE, lineitem.L_DISCOUNT).alias("value"))\
        .groupBy(nation.N_NAME)\
        .agg(F.sum("value").alias("revenue")).show()
		
		
		