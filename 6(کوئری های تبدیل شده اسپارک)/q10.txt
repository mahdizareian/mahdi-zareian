from decimal import *
from pyspark.sql.types import *
from pyspark.sql import functions as F
from pyspark.sql import *

orders = sqlContext.read.parquet("hdfs://namenode:8020/mahdi-parquet-data/orders.parquet")
lineitem = sqlContext.read.parquet("hdfs://namenode:8020/mahdi-parquet-data/lineitem.parquet")
customer = sqlContext.read.parquet("hdfs://namenode:8020/mahdi-parquet-data/customer.parquet")
nation = sqlContext.read.parquet("hdfs://namenode:8020/mahdi-parquet-data/nation.parquet")

func1 = lambda x , y : x * (1-y)

orders_filter = orders.filter((orders.O_ORDERDATE >= "1997-7-7") & (orders.O_ORDERDATE < "1997-7-10"))\



query10 = lineitem.filter(lineitem.L_RETURNFLAG == "R")\
.join (lineitem, orders_filter.O_ORDERKEY == lineitem.L_ORDERKEY)\
.join (customer, orders_filter.O_CUSTKEY == customer.C_CUSTKEY)\
.join (customer, nation.N_NATIONKEY == customer.C_NATIONKEY)\
.select(customer.C_CUSTKEY , customer.C_NAME , func1(lineitem.L_EXTENDEDPRICE , lineitem.L_DISCOUNT).alias("value") , customer.C_ACCTBAL , nation.N_NAME , customer.C_ADDRESS , customer.C_PHONE , customer.C_COMMENT )\
.groupBy(customer.C_CUSTKEY, customer.C_NAME, customer.C_ACCTBAL, customer.C_PHONE, nation.N_NAME, customer.C_ADDRESS, customer.C_COMMENT)\
.agg(F.sum("value").alias("revenue"))\
.orderBY("revenue" , ascending=False ).show()
	
..................................
ye moshkeli to join hast 
