from decimal import *
from pyspark.sql.types import *
from pyspark.sql import functions as F
from pyspark.sql import *

func1 = lambda x , y : x * (1-y)


customer = sqlContext.read.parquet("hdfs://namenode:8020/mahdi-parquet-data/customer.parquet")
orders = sqlContext.read.parquet("hdfs://namenode:8020/mahdi-parquet-data/orders.parquet")
linitem = sqlContext.read.parquet("hdfs://namenode:8020/mahdi-parquet-data/lineitem.parquet")

lineitem_filter = lineitem.filter(lineitem.L_SHIPDATE > "1997.6.8")\
.select(lineitem.L_ORDERKEY , lineitem.L_EXTENDEDPRICE , lineitem.L_DISCOUNT)

query3= orders.filter(orders.O_ORDERDATE < "1997.6.8")\
.join(customer, orders.O_CUSTKEY == customer.C_CUSTKEY)\
.join(lineitem_filter, orders.O_ORDERKEY == lineitem_filter.L_ORDERKEY)\
.select(lineitem_filter.L_ORDERKEY , func1(lineitem_filter.L_EXTENDEDPRICE , lineitem_filter.L_DISCOUNT).alias("value") , orders.O_ORDERDATE , orders.O_SHIPPRIORITY )\
.groupBy(lineitem_filter.L_ORDERKEY , orders.O_ORDERKEY , orders.SHIPPRIORITY)\
.agg(F.sum("value").alias("revenue"))\
.orderBy("revenue" , orders.O_ORDERDATE).show()



