from decimal import *
from pyspark.sql.types import *
from pyspark.sql import functions as F
from pyspark.sql import *

lineitem = sqlContext.read.parquet("hdfs://namenode:8020/mahdi-parquet-data/lineitem.parquet")
part = sqlContext.read.parquet("hdfs://namenode:8020/mahdi-parquet-data/part.parquet")

func1 = lambda x , y : x * (1 - y)

selin=join(lineitem, part.P_PARTKEY == lineitem.L_PARTKEY)\
.select( F.when(part.P_TYPE.like('PROMO%') , func1(lineitem.L_EXTENDEDPRICE , lineitem.L_DISCOUNT ).alias("value")).otherwise(0))\
.agg(F.sum("value").alias("promo_revenue"))\

query14 = lineitem.filter((lineitem.L_SHIPDATE >= '1997.1.2') & (lineitem.L_SHIPDATE< '1997.1.3'))\
.join(lineitem, part.P_PARTKEY == lineitem.L_PARTKEY)\
.select( 100000 * selin.promo_revenue)





****************************************************************

>>> df.select(df.name, F.when(df.age > 3, 1).otherwise(0)).show()
+-----+-------------------------------------+
| name|CASE WHEN (age > 3) THEN 1 ELSE 0 END|
+-----+-------------------------------------+
|Alice|                                    0|
|  Bob|                                    1|
