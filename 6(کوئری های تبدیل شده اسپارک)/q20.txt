from decimal import *
from pyspark.sql.types import *
from pyspark.sql import functions as F
from pyspark.sql import *

lineitem = sqlContext.read.parquet("hdfs://namenode:8020/mahdi-parquet-data/lineitem.parquet")
part = sqlContext.read.parquet("hdfs://namenode:8020/mahdi-parquet-data/part.parquet")
supplier = sqlContext.read.parquet("hdfs://namenode:8020/mahdi-parquet-data/supplier.parquet")
nation = sqlContext.read.parquet("hdfs://namenode:8020/mahdi-parquet-data/nation.parquet")
partsupp = sqlContext.read.parquet("hdfs://namenode:8020/mahdi-parquet-data/partsupp.parquet")

nation_filter = nation.filter(nation.N_NAME == "IRAN")

sellineitem = lineitem.filter((lineitem.L_SHIPDATE >= "1997-2-3") &(lineitem.L_SHIPDATE < "1998-2-3")) \
.groupBy(lineitem.L_PARTKEY, lineitem.L_SUPPKEY) \
.agg((F.sum(lineitem.L_QUANTITY) * 0.5).alias("sum_quantity"))

join_supp_nation = supplier.select(supplier.S_SUPPKEY, supplier.S_NAME, supplier.S_NATIONKEY, supplier.S_ADDRESS) \
 .join(nation_filter, supplier.S_NATIONKEY == nation_filter.N_NATIONKEY)


query20 = part.filter(part.P_NAME.like("tan%"))\
.select(part.P_PARTKEY).distinct()\
.join(part, partsupp.PS_PARTKEY == part.P_PARTKEY)\
.join(sellineitem , (partsupp.PS_SUPPKEY == sellineitem.L_SUPPKEY) & ( partsupp.PS_PARTKEY == sellineitem.L_PARTKEY))\
.filter(partsupp.PS_AVAILQTY > sellineitem.sum_quantity)\
.select(partsupp.PS_SUPPKEY).distinct()\
.join(join_supp_nation , partsupp.PS_PARTKEY == join_supp_nation.S_SUPPKEY)\
.select(join_supp_nation.S_NAME , join_supp_nation.S_ADDRESS)\
.orderBy(join_supp_nation.S_NAME)