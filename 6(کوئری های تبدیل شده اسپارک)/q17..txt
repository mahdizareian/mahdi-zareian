from decimal import *
from pyspark.sql.types import *
from pyspark.sql import functions as F
from pyspark.sql import *

part = sqlContext.read.parquet("hdfs://namenode:8020/mahdi-parquet-data/part.parquet")
lineitem = sqlContext.read.parquet("hdfs://namenode:8020/mahdi-parquet-data/lineitem.parquet")

func1= lambda x : x / 7

selin =lineitem.select( func1(lineitem.L_QUANTITY).alias("invalue") , lineitem.L_PARTKEY  , lineitem.L_EXTENDEDPRICE)\
.agg(F.avg("invalue").alias("outvalue"))

part_filter = 
query17 = part.filter((part.P_BRAND == "BRAND#35") & (part.P_CONTAINTER == "LG JAR"))\
.join(part , selin.L_PARTKEY == part.P_PARTKEY )
.join(lineitem, selin.L_PARTKEY ==  lineitem.L_PARTKEY)\
.filter(lineitem.L_QUANTITY < selin.outvalue)\
.select( selin.L_EXTENDEDPRICE) \
.agg(F.sum((selin.L_EXTENDEDPRICE) / 7).alias("avg_yearly"))\


...............................
masalan ghesmate selin be tanhayi ejra mishe !