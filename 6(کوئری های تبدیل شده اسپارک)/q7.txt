from decimal import *
from pyspark.sql.types import *
from pyspark.sql import functions as F
from pyspark.sql import *

nation = sqlContext.read.parquet("hdfs://namenode:8020/mahdi-parquet-data/nation.parquet")
lineitem = sqlContext.read.parquet("hdfs://namenode:8020/mahdi-parquet-data/lineitem.parquet")
orders = sqlContext.read.parquet("hdfs://namenode:8020/mahdi-parquet-data/orders.parquet")
customer = sqlContext.read.parquet("hdfs://namenode:8020/mahdi-parquet-data/customer.parquet")
supplier = sqlContext.read.parquet("hdfs://namenode:8020/mahdi-parquet-data/supplier.parquet")

func1 = lambda x , y : x * (1-y)

Year = lambda x : x[0: 4]

n1 = nation.alias ("n1")
n2 = nation.alias ("n2")

india_china =

china_india = 

selin = lineitem.filter((lineitem.L_SHIPDATE > "1995-01-01") & ( lineitem.L_SHIPDATE< "1996-12-31"))\
.join (supplier, lineitem.L_SUPPKEY == supplier.L_SUPPKEY )\
.join (orders, lineitem.L_ORDERKEY == orders.O_ORDERKEY )\
.join (customer , orders.O_CUSTKEY == customer.C_CUSTKEY)\
.join (supplier, n1.N_NATIONKEY == supplier.S_NATIONKEY)\
.join (customer, n2.N_NATIONKEY == customer.C_NATIONKEY)\
.select(n1.N_NAME.alias("supp_nation") , n2.N_NAME.alias("cust_nation") , fun1 (lineitem.L_EXTENDEDPRICE ,                                 .alias("volume"),               
  lineitem.L_DISCOUNT).alias("volume")).alias("shipping")\

						   
query7 = 	
.select(selin.supp_nation , selin.cust_nation , selin.l_year, selin.volume)\
.groupBy(selin.supp_nation , selin.cust_nation , selin.l_year)\
.agg(F.sum(selin.volume).alias("revenue"))\
.orderBy(selin.supp_nation , selin.cust_nation , selin.l_year)\						

**** piade kardane (n1.n_name = 'INDIA' and n2.n_name = 'CHINA')
				or (n1.n_name = 'CHINA' and n2.n_name = 'INDIA') *****
				
				