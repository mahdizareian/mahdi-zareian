from decimal import *
from pyspark.sql.types import *
from pyspark.sql import functions as F
from pyspark.sql import *

lineitem = sqlContext.read.parquet("hdfs://namenode:8020/mahdi-parquet-data/lineitem.parquet")

func1=lambda x ,y : x*y

lineitem_filter_1 = lineitem.filter((lineitem.L_SHIPDATE >= "1997-6-9") & (lineitem.L_SHIPDATE < "1998-6-9"))\
.select(lineitem.L_DISCOUNT , lineitem.L_EXTENDEDPRICE , lineitem.L_QUANTITY , lineitem.L_SHIPDATE )

query6 = lineitem.filter((0.02 <lineitem.L_DISCOUNT< 2.01) & (lineitem.L_QUANTITY < 17 ))\
.join(lineitem_filter_1 , lineitem.L_DISCOUNT == lineitem_filter_1.L_DISCOUNT )\
.select(func1(lineitem.L_EXTENDEDPRICE , lineitem.L_DISCOUNT).alias("revenue"))\
.agg(F.sum("revenue").alias("sumrevenue")).show()


...... 
lineitem_filter_1 ; okeye & ejra mishe .. ghesmate dovom nemidonam chera error bool mide


rah : to query6 ro lineitem_filter_1 filter ro test konam va join nakonam !